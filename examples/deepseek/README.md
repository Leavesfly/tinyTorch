# DeepSeek V3 ç¤ºä¾‹

è¿™ä¸ªç›®å½•åŒ…å« DeepSeek V3 è¯­è¨€æ¨¡å‹çš„ç®€åŒ–å®ç°ç¤ºä¾‹ã€‚

## æ–‡ä»¶è¯´æ˜

- `deepseek_v3_demo.py` - DeepSeek V3 å®Œæ•´ç¤ºä¾‹ï¼ŒåŒ…å« MoE æ¶æ„å®ç°å’Œè®­ç»ƒæµç¨‹

## DeepSeek V3 ç®€ä»‹

DeepSeek V3 æ˜¯ä¸€ä¸ªä½¿ç”¨ **MoE (Mixture of Experts)** æ¶æ„çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚

### æ ¸å¿ƒç‰¹æ€§

1. **MoE æ¶æ„**
   - ä½¿ç”¨ 256 ä¸ªä¸“å®¶ç½‘ç»œ
   - æ¯æ¬¡æ¨ç†åªæ¿€æ´» 8 ä¸ªä¸“å®¶
   - æ€»å‚æ•° 671Bï¼Œæ¿€æ´»å‚æ•°çº¦ 37B

2. **Multi-head Latent Attention (MLA)**
   - ä½¿ç”¨ä½ç§©åˆ†è§£å‹ç¼© KV ç¼“å­˜
   - é™ä½æ˜¾å­˜å ç”¨
   - æé«˜æ¨ç†æ•ˆç‡

3. **åŠ¨æ€é—¨æ§ç½‘ç»œ**
   - æ ¹æ®è¾“å…¥åŠ¨æ€é€‰æ‹©ä¸“å®¶
   - è´Ÿè½½å‡è¡¡æœºåˆ¶
   - ç¨€ç–æ¿€æ´»

## è¿è¡Œç¤ºä¾‹

```bash
# æ–¹æ³• 1ï¼šè®¾ç½® PYTHONPATH
cd tinyTorch
PYTHONPATH=. python examples/deepseek/deepseek_v3_demo.py

# æ–¹æ³• 2ï¼šå®‰è£… tinyTorch
cd tinyTorch
pip install -e .
python examples/deepseek/deepseek_v3_demo.py
```

## ç¤ºä¾‹è¾“å‡º

### 1. æ¨¡å‹åˆå§‹åŒ–

```
======================================================================
DeepSeek V3 è¯­è¨€æ¨¡å‹ç¤ºä¾‹
======================================================================

ã€è¯´æ˜ã€‘
DeepSeek V3 æ˜¯ä¸€ä¸ªä½¿ç”¨ MoE æ¶æ„çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚
æœ¬ç¤ºä¾‹å®ç°äº†ä¸€ä¸ªæç®€ç‰ˆæœ¬ï¼Œç”¨äºæ¼”ç¤ºæ ¸å¿ƒæ¦‚å¿µï¼š
  1. MoE (Mixture of Experts) - æ··åˆä¸“å®¶æ¶æ„
  2. Multi-head Latent Attention - å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›
  3. é—¨æ§ç½‘ç»œ - åŠ¨æ€é€‰æ‹©ä¸“å®¶
  4. ä½ç§©åˆ†è§£ - KV ç¼“å­˜å‹ç¼©

======================================================================
1. åˆ›å»º DeepSeek V3 æ¨¡å‹
======================================================================

åˆå§‹åŒ– DeepSeek V3 æ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š
  - è¯æ±‡è¡¨å¤§å°: 100
  - éšè—å±‚å¤§å°: 64
  - å±‚æ•°: 2
  - æ³¨æ„åŠ›å¤´æ•°: 4
  - ä¸“å®¶æ•°é‡: 4
  - æ¿€æ´»ä¸“å®¶æ•°: 2
âœ“ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ
```

### 2. å‰å‘ä¼ æ’­æµ‹è¯•

```
======================================================================
2. æµ‹è¯•å‰å‘ä¼ æ’­
======================================================================

è¾“å…¥ token IDs: [1, 2, 3, 4, 5]
æ‰§è¡Œå‰å‘ä¼ æ’­...
âœ“ è¾“å‡º logits å½¢çŠ¶: [1, 100]
  å‰ 5 ä¸ª logits: [0.023, -0.015, 0.008, 0.031, -0.012]
```

### 3. æ–‡æœ¬ç”Ÿæˆæµ‹è¯•

```
======================================================================
3. æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
======================================================================

æç¤º token IDs: [1, 2, 3]
ç”Ÿæˆæ–°çš„ tokens...
âœ“ ç”Ÿæˆçš„å®Œæ•´åºåˆ—: [1, 2, 3, 45, 23, 67, 12, 89]
  æ–°ç”Ÿæˆçš„éƒ¨åˆ†: [45, 23, 67, 12, 89]
```

### 4. è®­ç»ƒè¿‡ç¨‹

```
======================================================================
4. è®­ç»ƒæ¨¡å‹ç¤ºä¾‹
======================================================================

åˆ›å»ºè®­ç»ƒæ•°æ®é›†...
âœ“ æ•°æ®é›†åˆ›å»ºå®Œæˆï¼ŒåŒ…å« 20 ä¸ªæ ·æœ¬

æ ·æœ¬ç¤ºä¾‹ï¼š
  è¾“å…¥åºåˆ—: [1, 2, 3, 4]
  ç›®æ ‡åºåˆ—: [2, 3, 4, 5]

å¼€å§‹è®­ç»ƒï¼ˆç®€åŒ–æ¼”ç¤ºï¼‰...

======================================================================
å¼€å§‹è®­ç»ƒ DeepSeek V3 æ¨¡å‹
======================================================================

è®­ç»ƒé…ç½®ï¼š
  - æ•°æ®é›†å¤§å°: 20
  - è®­ç»ƒè½®æ•°: 2
  - å­¦ä¹ ç‡: 0.001

Epoch 1/2
----------------------------------------------------------------------
  æ‰¹æ¬¡ 10/20, å¹³å‡æŸå¤±: 2.3456
  æ‰¹æ¬¡ 20/20, å¹³å‡æŸå¤±: 2.1234

âœ“ Epoch 1 å®Œæˆ, å¹³å‡æŸå¤±: 2.1234

Epoch 2/2
----------------------------------------------------------------------
  æ‰¹æ¬¡ 10/20, å¹³å‡æŸå¤±: 1.9876
  æ‰¹æ¬¡ 20/20, å¹³å‡æŸå¤±: 1.8543

âœ“ Epoch 2 å®Œæˆ, å¹³å‡æŸå¤±: 1.8543

======================================================================
âœ“ è®­ç»ƒå®Œæˆï¼
======================================================================
```

### 5. è®­ç»ƒåç”Ÿæˆ

```
======================================================================
5. è®­ç»ƒåç”Ÿæˆæµ‹è¯•
======================================================================

æµ‹è¯•æç¤º: [1, 2, 3]
ç”Ÿæˆæ–°åºåˆ—...
âœ“ ç”Ÿæˆåºåˆ—: [1, 2, 3, 4, 5, 6, 7, 8]
  æ–°ç”Ÿæˆéƒ¨åˆ†: [4, 5, 6, 7, 8]
```

## ä»£ç ç»“æ„

### 1. MoEGate - é—¨æ§ç½‘ç»œ

```python
class MoEGate(Module):
    """MoE é—¨æ§ç½‘ç»œï¼Œç”¨äºé€‰æ‹©ä¸“å®¶å’Œè®¡ç®—æƒé‡ã€‚"""
    
    def forward(self, x):
        # è®¡ç®—é—¨æ§ logits
        gate_logits = self.gate(x)
        
        # Softmax å½’ä¸€åŒ–
        gate_weights = self._softmax(gate_logits)
        
        # Top-K é€‰æ‹©ä¸“å®¶
        expert_indices, expert_weights = self._top_k(gate_weights)
        
        return expert_indices, expert_weights
```

### 2. Expert - ä¸“å®¶ç½‘ç»œ

```python
class Expert(Module):
    """å•ä¸ªä¸“å®¶ç½‘ç»œï¼Œå®ç°ä¸ºå‰é¦ˆç½‘ç»œã€‚"""
    
    def forward(self, x):
        h = self.fc1(x)      # ç¬¬ä¸€å±‚
        h = self._relu(h)    # æ¿€æ´»
        output = self.fc2(h) # ç¬¬äºŒå±‚
        return output
```

### 3. DeepSeekMoE - MoE å±‚

```python
class DeepSeekMoE(Module):
    """DeepSeek MoE å±‚ï¼Œä½¿ç”¨æ··åˆä¸“å®¶æ¶æ„ã€‚"""
    
    def forward(self, x):
        # é—¨æ§é€‰æ‹©ä¸“å®¶
        expert_indices, weights = self.gate(x)
        
        # è®¡ç®—ä¸“å®¶è¾“å‡º
        expert_outputs = [
            self.experts[idx](x) for idx in expert_indices
        ]
        
        # åŠ æƒç»„åˆ
        output = self._combine(expert_outputs, weights)
        return output
```

### 4. MultiLatentAttention - MLA

```python
class MultiLatentAttention(Module):
    """Multi-head Latent Attentionï¼Œä½¿ç”¨ KV å‹ç¼©ã€‚"""
    
    def forward(self, x):
        # Q æŠ•å½±
        Q = self.q_proj(x)
        
        # KV ä½ç§©å‹ç¼©
        kv_compressed = self.kv_a_proj(x)  # å‹ç¼©
        KV = self.kv_b_proj(kv_compressed)  # æ¢å¤
        
        # æ³¨æ„åŠ›è®¡ç®—
        attention_out = self._attention(Q, KV)
        
        # è¾“å‡ºæŠ•å½±
        output = self.o_proj(attention_out)
        return output
```

### 5. DeepSeekV3Block - Transformer å—

```python
class DeepSeekV3Block(Module):
    """DeepSeek V3 Transformer å—ã€‚"""
    
    def forward(self, x):
        # Attention + æ®‹å·®
        x = x + self.attention(x)
        x = self.norm1(x)
        
        # MoE + æ®‹å·®
        x = x + self.moe(x)
        x = self.norm2(x)
        
        return x
```

### 6. DeepSeekV3Model - å®Œæ•´æ¨¡å‹

```python
class DeepSeekV3Model(Module):
    """DeepSeek V3 è¯­è¨€æ¨¡å‹ã€‚"""
    
    def __init__(self, vocab_size, hidden_size, num_layers, 
                 num_experts, top_k):
        # è¯åµŒå…¥
        self.embedding = Embedding(vocab_size, hidden_size)
        
        # DeepSeek V3 å±‚
        self.layers = [
            DeepSeekV3Block(...) for _ in range(num_layers)
        ]
        
        # è¾“å‡ºæŠ•å½±
        self.output_proj = Linear(hidden_size, vocab_size)
    
    def forward(self, input_ids):
        x = self.embedding(input_ids)
        
        for layer in self.layers:
            x = layer(x)
        
        logits = self.output_proj(x)
        return logits
    
    def generate(self, prompt_ids, max_new_tokens=10):
        """ç”Ÿæˆæ–‡æœ¬åºåˆ—ã€‚"""
        # è´ªå¿ƒè§£ç å®ç°
        ...
```

### 7. SimpleTextDataset - è®­ç»ƒæ•°æ®é›†

```python
class SimpleTextDataset:
    """ç®€å•çš„æ–‡æœ¬æ•°æ®é›†ï¼Œç”¨äºæ¼”ç¤ºè®­ç»ƒã€‚"""
    
    def __init__(self, sequences, vocab_size=100):
        self.sequences = sequences
        self.vocab_size = vocab_size
    
    def __getitem__(self, idx):
        seq = self.sequences[idx]
        # è¾“å…¥æ˜¯åºåˆ—çš„å‰ n-1 ä¸ª token
        input_ids = seq[:-1]
        # ç›®æ ‡æ˜¯åºåˆ—çš„å n-1 ä¸ª tokenï¼ˆè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼‰
        target_ids = seq[1:]
        return input_ids, target_ids
```

### 8. è®­ç»ƒå‡½æ•°

```python
def train_step(model, input_ids, target_ids, optimizer, loss_fn):
    """æ‰§è¡Œä¸€æ¬¡è®­ç»ƒæ­¥éª¤ã€‚"""
    # å‰å‘ä¼ æ’­
    logits = model.forward(input_var)
    
    # è®¡ç®—æŸå¤±
    loss = calculate_simple_loss(logits, target_id)
    
    # åå‘ä¼ æ’­
    # ...
    
    return loss

def train_model(model, dataset, num_epochs=3, learning_rate=0.001):
    """è®­ç»ƒ DeepSeek V3 æ¨¡å‹ã€‚"""
    for epoch in range(num_epochs):
        for idx in range(len(dataset)):
            input_ids, target_ids = dataset[idx]
            loss = train_step(model, input_ids, target_ids, optimizer, loss_fn)
            # è®°å½•å’Œæ˜¾ç¤ºè®­ç»ƒè¿›åº¦
            ...
```

## è®­ç»ƒæµç¨‹

### æ•°æ®å‡†å¤‡

```python
# åˆ›å»ºè®­ç»ƒåºåˆ—
training_sequences = [
    [1, 2, 3, 4, 5],
    [2, 3, 4, 5, 6],
    [3, 4, 5, 6, 7],
    # ... æ›´å¤šåºåˆ—
]

# åˆ›å»ºæ•°æ®é›†
dataset = SimpleTextDataset(training_sequences, vocab_size=100)
```

### è®­ç»ƒé…ç½®

```python
# æ¨¡å‹å‚æ•°
model = DeepSeekV3Model(
    vocab_size=100,
    hidden_size=64,
    num_layers=2,
    num_heads=4,
    intermediate_size=256,
    num_experts=4,
    top_k=2
)

# è®­ç»ƒè¶…å‚æ•°
num_epochs = 3
learning_rate = 0.001
```

### è®­ç»ƒå¾ªç¯

```python
# å¼€å§‹è®­ç»ƒ
train_model(model, dataset, num_epochs=2, learning_rate=0.001)

# è®­ç»ƒè¾“å‡ºç¤ºä¾‹ï¼š
# Epoch 1/2
# ----------------------------------------------------------------------
#   æ‰¹æ¬¡ 10/20, å¹³å‡æŸå¤±: 2.3456
#   æ‰¹æ¬¡ 20/20, å¹³å‡æŸå¤±: 2.1234
# âœ“ Epoch 1 å®Œæˆ, å¹³å‡æŸå¤±: 2.1234
```

### è®­ç»ƒåè¯„ä¼°

```python
# æµ‹è¯•ç”Ÿæˆæ•ˆæœ
test_prompt = [1, 2, 3]
generated = model.generate(test_prompt, max_new_tokens=5)
print(f"ç”Ÿæˆåºåˆ—: {generated}")
```

## æ¨¡å‹è§„æ ¼å¯¹æ¯”

| å‚æ•° | æœ¬ç¤ºä¾‹ï¼ˆæ•™å­¦ç‰ˆï¼‰| DeepSeek V3ï¼ˆå®é™…ï¼‰|
|------|----------------|-------------------|
| è¯æ±‡è¡¨ | 100 | 102,400 |
| éšè—å±‚ | 64 | 7,168 |
| å±‚æ•° | 2 | 61 |
| æ³¨æ„åŠ›å¤´ | 4 | 128 |
| ä¸“å®¶æ•°é‡ | 4 | 256 |
| æ¿€æ´»ä¸“å®¶ | 2 | 8 |
| æ€»å‚æ•° | ~50K | 671B |
| æ¿€æ´»å‚æ•° | ~25K | 37B |

## æ ¸å¿ƒæ¦‚å¿µ

### MoE (Mixture of Experts)

**ä¼˜åŠ¿**ï¼š
- âœ… æ¨¡å‹å®¹é‡å¤§ï¼šå¯ä»¥ä½¿ç”¨æ›´å¤šå‚æ•°
- âœ… æ¨ç†é«˜æ•ˆï¼šæ¯æ¬¡åªæ¿€æ´»éƒ¨åˆ†ä¸“å®¶
- âœ… ä¸“ä¸šåŒ–ï¼šä¸åŒä¸“å®¶å­¦ä¹ ä¸åŒæ¨¡å¼

**å·¥ä½œæµç¨‹**ï¼š
```
è¾“å…¥ â†’ é—¨æ§ç½‘ç»œ â†’ é€‰æ‹© Top-K ä¸“å®¶ â†’ ä¸“å®¶è®¡ç®— â†’ åŠ æƒç»„åˆ â†’ è¾“å‡º
```

### Multi-head Latent Attention

**ä¼˜åŠ¿**ï¼š
- âœ… æ˜¾å­˜èŠ‚çœï¼šä½¿ç”¨ä½ç§©åˆ†è§£å‹ç¼© KV ç¼“å­˜
- âœ… è®¡ç®—é«˜æ•ˆï¼šé™ä½æ³¨æ„åŠ›å¤æ‚åº¦
- âœ… æ€§èƒ½ä¿æŒï¼šä¿æŒæ¨¡å‹è¡¨è¾¾èƒ½åŠ›

**KV å‹ç¼©**ï¼š
```
åŸå§‹ KV: [batch, seq_len, hidden_dim]
          â†“ ä½ç§©æŠ•å½± (hidden_dim â†’ rank)
å‹ç¼© KV: [batch, seq_len, rank]
          â†“ æ¢å¤æŠ•å½± (rank â†’ hidden_dim)
æ¢å¤ KV: [batch, seq_len, hidden_dim]
```

## å­¦ä¹ è·¯å¾„

### åˆçº§ï¼šç†è§£åŸºç¡€æ¦‚å¿µ
1. é˜…è¯»ä»£ç æ³¨é‡Š
2. ç†è§£ MoE çš„å·¥ä½œåŸç†
3. äº†è§£é—¨æ§ç½‘ç»œçš„ä½œç”¨

### ä¸­çº§ï¼šæ·±å…¥å®ç°ç»†èŠ‚
1. ç ”ç©¶ä½ç§©åˆ†è§£å¦‚ä½•å·¥ä½œ
2. ç†è§£ä¸“å®¶å¦‚ä½•è¢«é€‰æ‹©
3. æ¢ç´¢è´Ÿè½½å‡è¡¡æœºåˆ¶

### é«˜çº§ï¼šæ‰©å±•å’Œä¼˜åŒ–
1. å¢åŠ ä¸“å®¶æ•°é‡
2. å®ç°æ›´å¤æ‚çš„é—¨æ§ç­–ç•¥
3. ä¼˜åŒ–å†…å­˜å’Œè®¡ç®—æ•ˆç‡
4. å®ç°å®Œæ•´çš„è®­ç»ƒæµç¨‹
5. æ·»åŠ æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
6. å®ç°æ‰¹é‡è®­ç»ƒå’Œæ•°æ®åŠ è½½å™¨

## æ‰©å±•æ€è·¯

### 1. å¢å¼ºé—¨æ§ç½‘ç»œ
```python
# æ·»åŠ å™ªå£°ä»¥æ”¹å–„è´Ÿè½½å‡è¡¡
gate_logits = gate_logits + noise

# ä½¿ç”¨ Top-K + Softmax
top_k_logits = select_top_k(gate_logits, k)
weights = softmax(top_k_logits)
```

### 2. è´Ÿè½½å‡è¡¡
```python
# æ·»åŠ è¾…åŠ©æŸå¤±
load_balance_loss = compute_load_balance(expert_usage)
total_loss = main_loss + alpha * load_balance_loss
```

### 3. åŠ¨æ€ä¸“å®¶æ•°é‡
```python
# æ ¹æ®è¾“å…¥å¤æ‚åº¦è°ƒæ•´æ¿€æ´»çš„ä¸“å®¶æ•°
k = compute_adaptive_k(input_complexity)
expert_indices = select_top_k(gate_weights, k)
```

### 4. å®Œæ•´è®­ç»ƒæµç¨‹
```python
# æ·»åŠ ä¼˜åŒ–å™¨
from tinytorch.ml.optimizers import Adam
optimizer = Adam(model.parameters(), lr=0.001)

# æ·»åŠ æŸå¤±å‡½æ•°
from tinytorch.ml.losses import CrossEntropyLoss
loss_fn = CrossEntropyLoss()

# è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    for batch in dataloader:
        optimizer.zero_grad()
        loss = compute_loss(model(batch))
        loss.backward()
        optimizer.step()
```

## ç›¸å…³èµ„æº

- [DeepSeek V3 æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2401.xxxxx)
- [tinyTorch æ•™ç¨‹](../../tutorials/README.md)
- [Transformer ç¤ºä¾‹](../transformer/simple_transformer.py)
- [MoE æ¶æ„è¯¦è§£](../../docs/æ–¹æ¡ˆ.md)

## å¸¸è§é—®é¢˜

**Q: ä¸ºä»€ä¹ˆä½¿ç”¨ MoEï¼Ÿ**  
A: MoE å…è®¸æ¨¡å‹æ‹¥æœ‰æ›´å¤šå‚æ•°ï¼ˆæå‡å®¹é‡ï¼‰ï¼Œä½†æ¨ç†æ—¶åªæ¿€æ´»éƒ¨åˆ†å‚æ•°ï¼ˆä¿æŒæ•ˆç‡ï¼‰ã€‚

**Q: å¦‚ä½•é€‰æ‹©ä¸“å®¶æ•°é‡ï¼Ÿ**  
A: å–å†³äºä»»åŠ¡å¤æ‚åº¦å’Œè®¡ç®—èµ„æºã€‚æ›´å¤šä¸“å®¶ = æ›´å¤§å®¹é‡ï¼Œä½†éœ€è¦æ›´å¤šæ˜¾å­˜ã€‚

**Q: é—¨æ§ç½‘ç»œå¦‚ä½•è®­ç»ƒï¼Ÿ**  
A: é—¨æ§ç½‘ç»œé€šè¿‡æ¢¯åº¦åå‘ä¼ æ’­è‡ªåŠ¨å­¦ä¹ ï¼Œå­¦ä¼šä¸ºä¸åŒè¾“å…¥é€‰æ‹©åˆé€‚çš„ä¸“å®¶ã€‚

**Q: å¦‚ä½•é¿å…ä¸“å®¶è´Ÿè½½ä¸å‡ï¼Ÿ**  
A: ä½¿ç”¨è¾…åŠ©æŸå¤±å‡½æ•°é¼“åŠ±å‡è¡¡ä½¿ç”¨æ‰€æœ‰ä¸“å®¶ï¼Œæˆ–æ·»åŠ è´Ÿè½½å‡è¡¡çº¦æŸã€‚

**Q: å¦‚ä½•è®­ç»ƒ DeepSeek V3 æ¨¡å‹ï¼Ÿ**  
A: ç¤ºä¾‹ä¸­åŒ…å«äº†å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®é›†å‡†å¤‡ã€è®­ç»ƒå¾ªç¯ã€æŸå¤±è®¡ç®—å’Œè®­ç»ƒè¿›åº¦ç›‘æ§ã€‚

**Q: è®­ç»ƒæ•°æ®å¦‚ä½•å‡†å¤‡ï¼Ÿ**  
A: ä½¿ç”¨ SimpleTextDataset ç±»ï¼Œå°†æ–‡æœ¬åºåˆ—è½¬æ¢ä¸ºè¾“å…¥-ç›®æ ‡å¯¹ï¼Œç”¨äºè¯­è¨€å»ºæ¨¡ä»»åŠ¡ã€‚

**Q: å¦‚ä½•ç›‘æ§è®­ç»ƒè¿›åº¦ï¼Ÿ**  
A: ç¤ºä¾‹ä¸­æ¯ 10 ä¸ªæ‰¹æ¬¡è¾“å‡ºä¸€æ¬¡å¹³å‡æŸå¤±ï¼Œæ¯ä¸ª epoch ç»“æŸæ—¶æ˜¾ç¤ºæ•´ä½“ç»Ÿè®¡ä¿¡æ¯ã€‚

## è‡´è°¢

- æœ¬ç¤ºä¾‹åŸºäº DeepSeek V3 æŠ€æœ¯æŠ¥å‘Šå®ç°
- æ„Ÿè°¢ tinyTorch æ¡†æ¶æä¾›çš„åŸºç¡€ç»„ä»¶
- å‚è€ƒäº† PyTorch å’Œ Transformers åº“çš„å®ç°

---

**Happy Learning! ğŸš€**
