# DeepSeek V3 ç¤ºä¾‹

è¿™ä¸ªç›®å½•åŒ…å« DeepSeek V3 è¯­è¨€æ¨¡å‹çš„ç®€åŒ–å®ç°ç¤ºä¾‹ã€‚

## æ–‡ä»¶è¯´æ˜

- `deepseek_v3_demo.py` - DeepSeek V3 å®Œæ•´ç¤ºä¾‹ï¼ŒåŒ…å« MoE æ¶æ„å®ç°

## DeepSeek V3 ç®€ä»‹

DeepSeek V3 æ˜¯ä¸€ä¸ªä½¿ç”¨ **MoE (Mixture of Experts)** æ¶æ„çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚

### æ ¸å¿ƒç‰¹æ€§

1. **MoE æ¶æ„**
   - ä½¿ç”¨ 256 ä¸ªä¸“å®¶ç½‘ç»œ
   - æ¯æ¬¡æ¨ç†åªæ¿€æ´» 8 ä¸ªä¸“å®¶
   - æ€»å‚æ•° 671Bï¼Œæ¿€æ´»å‚æ•°çº¦ 37B

2. **Multi-head Latent Attention (MLA)**
   - ä½¿ç”¨ä½ç§©åˆ†è§£å‹ç¼© KV ç¼“å­˜
   - é™ä½æ˜¾å­˜å ç”¨
   - æé«˜æ¨ç†æ•ˆç‡

3. **åŠ¨æ€é—¨æ§ç½‘ç»œ**
   - æ ¹æ®è¾“å…¥åŠ¨æ€é€‰æ‹©ä¸“å®¶
   - è´Ÿè½½å‡è¡¡æœºåˆ¶
   - ç¨€ç–æ¿€æ´»

## è¿è¡Œç¤ºä¾‹

```bash
# æ–¹æ³• 1ï¼šè®¾ç½® PYTHONPATH
cd tinyTorch
PYTHONPATH=. python examples/deepseek/deepseek_v3_demo.py

# æ–¹æ³• 2ï¼šå®‰è£… tinyTorch
cd tinyTorch
pip install -e .
python examples/deepseek/deepseek_v3_demo.py
```

## ç¤ºä¾‹è¾“å‡º

```
======================================================================
DeepSeek V3 è¯­è¨€æ¨¡å‹ç¤ºä¾‹
======================================================================

ã€è¯´æ˜ã€‘
DeepSeek V3 æ˜¯ä¸€ä¸ªä½¿ç”¨ MoE æ¶æ„çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚
æœ¬ç¤ºä¾‹å®ç°äº†ä¸€ä¸ªæç®€ç‰ˆæœ¬ï¼Œç”¨äºæ¼”ç¤ºæ ¸å¿ƒæ¦‚å¿µï¼š
  1. MoE (Mixture of Experts) - æ··åˆä¸“å®¶æ¶æ„
  2. Multi-head Latent Attention - å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›
  3. é—¨æ§ç½‘ç»œ - åŠ¨æ€é€‰æ‹©ä¸“å®¶
  4. ä½ç§©åˆ†è§£ - KV ç¼“å­˜å‹ç¼©

======================================================================
1. åˆ›å»º DeepSeek V3 æ¨¡å‹
======================================================================

åˆå§‹åŒ– DeepSeek V3 æ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š
  - è¯æ±‡è¡¨å¤§å°: 100
  - éšè—å±‚å¤§å°: 64
  - å±‚æ•°: 2
  - æ³¨æ„åŠ›å¤´æ•°: 4
  - ä¸“å®¶æ•°é‡: 4
  - æ¿€æ´»ä¸“å®¶æ•°: 2
âœ“ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ
```

## ä»£ç ç»“æ„

### 1. MoEGate - é—¨æ§ç½‘ç»œ

```python
class MoEGate(Module):
    """MoE é—¨æ§ç½‘ç»œï¼Œç”¨äºé€‰æ‹©ä¸“å®¶å’Œè®¡ç®—æƒé‡ã€‚"""
    
    def forward(self, x):
        # è®¡ç®—é—¨æ§ logits
        gate_logits = self.gate(x)
        
        # Softmax å½’ä¸€åŒ–
        gate_weights = self._softmax(gate_logits)
        
        # Top-K é€‰æ‹©ä¸“å®¶
        expert_indices, expert_weights = self._top_k(gate_weights)
        
        return expert_indices, expert_weights
```

### 2. Expert - ä¸“å®¶ç½‘ç»œ

```python
class Expert(Module):
    """å•ä¸ªä¸“å®¶ç½‘ç»œï¼Œå®ç°ä¸ºå‰é¦ˆç½‘ç»œã€‚"""
    
    def forward(self, x):
        h = self.fc1(x)      # ç¬¬ä¸€å±‚
        h = self._relu(h)    # æ¿€æ´»
        output = self.fc2(h) # ç¬¬äºŒå±‚
        return output
```

### 3. DeepSeekMoE - MoE å±‚

```python
class DeepSeekMoE(Module):
    """DeepSeek MoE å±‚ï¼Œä½¿ç”¨æ··åˆä¸“å®¶æ¶æ„ã€‚"""
    
    def forward(self, x):
        # é—¨æ§é€‰æ‹©ä¸“å®¶
        expert_indices, weights = self.gate(x)
        
        # è®¡ç®—ä¸“å®¶è¾“å‡º
        expert_outputs = [
            self.experts[idx](x) for idx in expert_indices
        ]
        
        # åŠ æƒç»„åˆ
        output = self._combine(expert_outputs, weights)
        return output
```

### 4. MultiLatentAttention - MLA

```python
class MultiLatentAttention(Module):
    """Multi-head Latent Attentionï¼Œä½¿ç”¨ KV å‹ç¼©ã€‚"""
    
    def forward(self, x):
        # Q æŠ•å½±
        Q = self.q_proj(x)
        
        # KV ä½ç§©å‹ç¼©
        kv_compressed = self.kv_a_proj(x)  # å‹ç¼©
        KV = self.kv_b_proj(kv_compressed)  # æ¢å¤
        
        # æ³¨æ„åŠ›è®¡ç®—
        attention_out = self._attention(Q, KV)
        
        # è¾“å‡ºæŠ•å½±
        output = self.o_proj(attention_out)
        return output
```

### 5. DeepSeekV3Block - Transformer å—

```python
class DeepSeekV3Block(Module):
    """DeepSeek V3 Transformer å—ã€‚"""
    
    def forward(self, x):
        # Attention + æ®‹å·®
        x = x + self.attention(x)
        x = self.norm1(x)
        
        # MoE + æ®‹å·®
        x = x + self.moe(x)
        x = self.norm2(x)
        
        return x
```

### 6. DeepSeekV3Model - å®Œæ•´æ¨¡å‹

```python
class DeepSeekV3Model(Module):
    """DeepSeek V3 è¯­è¨€æ¨¡å‹ã€‚"""
    
    def __init__(self, vocab_size, hidden_size, num_layers, 
                 num_experts, top_k):
        # è¯åµŒå…¥
        self.embedding = Embedding(vocab_size, hidden_size)
        
        # DeepSeek V3 å±‚
        self.layers = [
            DeepSeekV3Block(...) for _ in range(num_layers)
        ]
        
        # è¾“å‡ºæŠ•å½±
        self.output_proj = Linear(hidden_size, vocab_size)
    
    def forward(self, input_ids):
        x = self.embedding(input_ids)
        
        for layer in self.layers:
            x = layer(x)
        
        logits = self.output_proj(x)
        return logits
```

## æ¨¡å‹è§„æ ¼å¯¹æ¯”

| å‚æ•° | æœ¬ç¤ºä¾‹ï¼ˆæ•™å­¦ç‰ˆï¼‰| DeepSeek V3ï¼ˆå®é™…ï¼‰|
|------|----------------|-------------------|
| è¯æ±‡è¡¨ | 100 | 102,400 |
| éšè—å±‚ | 64 | 7,168 |
| å±‚æ•° | 2 | 61 |
| æ³¨æ„åŠ›å¤´ | 4 | 128 |
| ä¸“å®¶æ•°é‡ | 4 | 256 |
| æ¿€æ´»ä¸“å®¶ | 2 | 8 |
| æ€»å‚æ•° | ~50K | 671B |
| æ¿€æ´»å‚æ•° | ~25K | 37B |

## æ ¸å¿ƒæ¦‚å¿µ

### MoE (Mixture of Experts)

**ä¼˜åŠ¿**ï¼š
- âœ… æ¨¡å‹å®¹é‡å¤§ï¼šå¯ä»¥ä½¿ç”¨æ›´å¤šå‚æ•°
- âœ… æ¨ç†é«˜æ•ˆï¼šæ¯æ¬¡åªæ¿€æ´»éƒ¨åˆ†ä¸“å®¶
- âœ… ä¸“ä¸šåŒ–ï¼šä¸åŒä¸“å®¶å­¦ä¹ ä¸åŒæ¨¡å¼

**å·¥ä½œæµç¨‹**ï¼š
```
è¾“å…¥ â†’ é—¨æ§ç½‘ç»œ â†’ é€‰æ‹© Top-K ä¸“å®¶ â†’ ä¸“å®¶è®¡ç®— â†’ åŠ æƒç»„åˆ â†’ è¾“å‡º
```

### Multi-head Latent Attention

**ä¼˜åŠ¿**ï¼š
- âœ… æ˜¾å­˜èŠ‚çœï¼šä½¿ç”¨ä½ç§©åˆ†è§£å‹ç¼© KV ç¼“å­˜
- âœ… è®¡ç®—é«˜æ•ˆï¼šé™ä½æ³¨æ„åŠ›å¤æ‚åº¦
- âœ… æ€§èƒ½ä¿æŒï¼šä¿æŒæ¨¡å‹è¡¨è¾¾èƒ½åŠ›

**KV å‹ç¼©**ï¼š
```
åŸå§‹ KV: [batch, seq_len, hidden_dim]
          â†“ ä½ç§©æŠ•å½± (hidden_dim â†’ rank)
å‹ç¼© KV: [batch, seq_len, rank]
          â†“ æ¢å¤æŠ•å½± (rank â†’ hidden_dim)
æ¢å¤ KV: [batch, seq_len, hidden_dim]
```

## å­¦ä¹ è·¯å¾„

### åˆçº§ï¼šç†è§£åŸºç¡€æ¦‚å¿µ
1. é˜…è¯»ä»£ç æ³¨é‡Š
2. ç†è§£ MoE çš„å·¥ä½œåŸç†
3. äº†è§£é—¨æ§ç½‘ç»œçš„ä½œç”¨

### ä¸­çº§ï¼šæ·±å…¥å®ç°ç»†èŠ‚
1. ç ”ç©¶ä½ç§©åˆ†è§£å¦‚ä½•å·¥ä½œ
2. ç†è§£ä¸“å®¶å¦‚ä½•è¢«é€‰æ‹©
3. æ¢ç´¢è´Ÿè½½å‡è¡¡æœºåˆ¶

### é«˜çº§ï¼šæ‰©å±•å’Œä¼˜åŒ–
1. å¢åŠ ä¸“å®¶æ•°é‡
2. å®ç°æ›´å¤æ‚çš„é—¨æ§ç­–ç•¥
3. ä¼˜åŒ–å†…å­˜å’Œè®¡ç®—æ•ˆç‡

## æ‰©å±•æ€è·¯

### 1. å¢å¼ºé—¨æ§ç½‘ç»œ
```python
# æ·»åŠ å™ªå£°ä»¥æ”¹å–„è´Ÿè½½å‡è¡¡
gate_logits = gate_logits + noise

# ä½¿ç”¨ Top-K + Softmax
top_k_logits = select_top_k(gate_logits, k)
weights = softmax(top_k_logits)
```

### 2. è´Ÿè½½å‡è¡¡
```python
# æ·»åŠ è¾…åŠ©æŸå¤±
load_balance_loss = compute_load_balance(expert_usage)
total_loss = main_loss + alpha * load_balance_loss
```

### 3. åŠ¨æ€ä¸“å®¶æ•°é‡
```python
# æ ¹æ®è¾“å…¥å¤æ‚åº¦è°ƒæ•´æ¿€æ´»çš„ä¸“å®¶æ•°
k = compute_adaptive_k(input_complexity)
expert_indices = select_top_k(gate_weights, k)
```

## ç›¸å…³èµ„æº

- [DeepSeek V3 æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2401.xxxxx)
- [tinyTorch æ•™ç¨‹](../../tutorials/README.md)
- [Transformer ç¤ºä¾‹](../transformer/simple_transformer.py)
- [MoE æ¶æ„è¯¦è§£](../../docs/æ–¹æ¡ˆ.md)

## å¸¸è§é—®é¢˜

**Q: ä¸ºä»€ä¹ˆä½¿ç”¨ MoEï¼Ÿ**  
A: MoE å…è®¸æ¨¡å‹æ‹¥æœ‰æ›´å¤šå‚æ•°ï¼ˆæå‡å®¹é‡ï¼‰ï¼Œä½†æ¨ç†æ—¶åªæ¿€æ´»éƒ¨åˆ†å‚æ•°ï¼ˆä¿æŒæ•ˆç‡ï¼‰ã€‚

**Q: å¦‚ä½•é€‰æ‹©ä¸“å®¶æ•°é‡ï¼Ÿ**  
A: å–å†³äºä»»åŠ¡å¤æ‚åº¦å’Œè®¡ç®—èµ„æºã€‚æ›´å¤šä¸“å®¶ = æ›´å¤§å®¹é‡ï¼Œä½†éœ€è¦æ›´å¤šæ˜¾å­˜ã€‚

**Q: é—¨æ§ç½‘ç»œå¦‚ä½•è®­ç»ƒï¼Ÿ**  
A: é—¨æ§ç½‘ç»œé€šè¿‡æ¢¯åº¦åå‘ä¼ æ’­è‡ªåŠ¨å­¦ä¹ ï¼Œå­¦ä¼šä¸ºä¸åŒè¾“å…¥é€‰æ‹©åˆé€‚çš„ä¸“å®¶ã€‚

**Q: å¦‚ä½•é¿å…ä¸“å®¶è´Ÿè½½ä¸å‡ï¼Ÿ**  
A: ä½¿ç”¨è¾…åŠ©æŸå¤±å‡½æ•°é¼“åŠ±å‡è¡¡ä½¿ç”¨æ‰€æœ‰ä¸“å®¶ï¼Œæˆ–æ·»åŠ è´Ÿè½½å‡è¡¡çº¦æŸã€‚

## è‡´è°¢

- æœ¬ç¤ºä¾‹åŸºäº DeepSeek V3 æŠ€æœ¯æŠ¥å‘Šå®ç°
- æ„Ÿè°¢ tinyTorch æ¡†æ¶æä¾›çš„åŸºç¡€ç»„ä»¶
- å‚è€ƒäº† PyTorch å’Œ Transformers åº“çš„å®ç°

---

**Happy Learning! ğŸš€**
